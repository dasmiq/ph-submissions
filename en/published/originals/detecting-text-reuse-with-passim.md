---
title: Detecting Text Reuse with Passim 2.0
collection: lessons
layout: lesson
slug: detecting-text-reuse-with-Passim
date: 2022-12-27
authors:
- Matteo Romanello
- Simon Hengchen
- David Smith
editors: Anna-Maria Sichani
reviewers: 
- Ryan Muther 
- Marco Büchler
review-ticket: https://github.com/programminghistorian/ph-submissions/issues/305
difficulty: 3
activity: transforming
topics: [text-reuse, data-manipulation]
abstract: In this lesson you will learn about text reuse detection -- the automatic identification of reused passages in texts -- and why you might want to use it in your research. Through a detailed installation guide and two case studies, this lesson will teach you the ropes of Passim, an open source and scalable tool for text reuse detection.
avatar_alt: Stack of newspapers surrounded by quills and telegraph wires
doi: 10.46430/phen0092
redirect_from: "/lessons/detecting-text-reuse-with-passim"
---

{% include toc.html %}

(Note: This tutorial, originally written by Matteo Romanello and Simon Hengchen, was updated by David Smith to cover the installation and use of Passim version 2.)

In this lesson you will be introduced to the automatic detection of text reuse with the Passim library. You will learn how to install and run Passim and its dependencies, how to prepare your texts as input files suitable for use with Passim and, finally, how to process the output generated by Passim to carry out basic analyses.

This lesson targets digital humanities (DH) practitioners without any prior knowledge of text reuse, but with a working knowledge of [bash scripting](https://en.wikipedia.org/wiki/Bash_(Unix_shell)) and Python as well as some data manipulation. For tutorials on bash scripting and [Python](https://en.wikipedia.org/wiki/Python_(programming_language)), you can refer to the Programming Historian [“Introduction to the Bash Command Line](https://programminghistorian.org/en/lessons/intro-to-bash) tutorial and the [library of current Python lessons](https://programminghistorian.org/en/lessons/?topic=python) on the *Programming Historian* website.

This lesson includes an overview of [Passim](https://github.com/dasmiq/passim), an open source tool for automatic text reuse detection. While the tool has been used in a number of small and large DH projects, it lacks a user-friendly documentation with examples and set up instructions, a gap that we aim to fill with this *Programming Historian* lesson.

# Introduction to Text Reuse

Text reuse can be defined as "the meaningful reiteration of text, usually beyond the simple repetition of common language" (Romanello et al. 2014). It is such a broad concept that it can be understood at different levels and studied in a large variety of contexts. In a publishing or teaching context, for example, instances of text reuse can constitute plagiarism should portions of someone else’s text be repeated without appropriate attribution. In the context of literary studies, text reuse is often just a synonym for literary phenomena like allusions, paraphrases and direct quotations.

The following list includes just some of the libraries available that perform automatic text reuse detection:

- The [R textreuse package](https://docs.ropensci.org/textreuse/) (R) written by Lincoln Mullen
- [TRACER](https://www.etrap.eu/research/tracer/) (Java) developed by Marco Büchler and colleagues
- [Basic Local Alignment Search Tool (BLAST)](https://blast.ncbi.nlm.nih.gov/Blast.cgi)
- [MatchMaker](https://github.com/JSTOR-Labs/matchmaker) (Python) developed by the JSTOR Labs
- [Tesserae](https://github.com/tesserae/tesserae) (PHP, Perl)
- [TextPAIR (Pairwise Alignment for Intertextual Relations)](https://github.com/ARTFL-Project/text-pair)
- [Passim](https://github.com/dasmiq/passim) (Python) developed by [David Smith](http://www.ccs.neu.edu/home/dasmith/) (Northeastern University)

For this tutorial we chose the Passim library for three main reasons. Firstly, it can be adapted to a variety of use cases as it works well on a small text collection as well as on a large-scale corpus. Secondly, while the documentation for Passim is extensive, because of its relatively advanced user audience, a more user-centered step-by-step tutorial about detecting text reuse with Passim would be beneficial to the user community. Lastly, the following examples illustrate the variety of scenarios in which text reuse is a useful methodology:

- To determine whether a digital library contains multiple editions of the same work(s)
- To find quotations in a text, provided that the target works are known (e.g. find quotations of the Bible within 17c English literature)  
- To study the virality and spread of texts (e.g. [Viral Texts](https://viraltexts.org/) by Cordell and Smith for historical newspapers)
- To identify (and possibly filter out) duplicate documents within a text collection before performing further processing steps (e.g., topic modelling as illustrated by Schofield et al. (2017))

For these reasons, Passim is usually a great choice. It will help you automate the search for repeated text passages in a corpus — whether these are running ads in newspapers, multiple copies of the same poem, or direct (and slightly indirect) quotations in someone else's book.
Text reuse detection as implemented in Passim aims at identifying these copies and repetitions automatically, and yields clusters of passages that were deemed to be related with one another. Ultimately, what a cluster contains can vary a lot and will depend on your research question. For example, Passim can group together copies of the same article that differ only with respect to optical character recognition (OCR) errors, but it can also help to retrieve texts that share the same journalistic template, such as horoscopes or advertisements.

# Prerequisites

This tutorial requires the following:
- A basic understanding of Bash scripts. For readers needing a review on Bash scripts, read the *Programming Historian* lesson ["Introduction to the Bash Command Line"](https://programminghistorian.org/en/lessons/intro-to-bash).
- Knowledge of JSON. To learn more about JSON, read the *Programming Historian* lesson ["Reshaping JSON with jq"](https://programminghistorian.org/en/lessons/json-and-jq).

Moreover, while a basic understanding of Python — and a working Python installation — are not strictly needed to work with Passim, they are required to run some parts of this tutorial (e.g. the Jupyter notebook with data exploration, or the Early English Books Online (EEBO) data preparation script). If you are not familiar with Python, please read the *Programming Historian* lesson ["Python Introduction and Installation"](https://programminghistorian.org/en/lessons/introduction-and-installation).   

Note that installing Passim on Windows is more arduous than macOS or Linux. As a result, we recommend using macOS or Linux (or a virtual environment) for this lesson.

# Installing Passim

Installing Passim requires first installing _either_ the Java Runtime Environment or the Java Development Kit.  Both of these options provide the `java` command.

But why are all these dependencies needed?

Since Passim is designed to work on large-scale text collections (with several thousands or millions of documents), behind the scenes it uses Apache Spark, a cluster-computing framework, which runs on the Java Virtual Machine. Using Spark allows Passim to handle the distributed processing of certain parts of the code, which is useful when handling large amounts of data. The [Spark glossary](https://spark.apache.org/docs/latest/cluster-overview.html#glossary) is a useful resource to learn basic Spark terminology (words like "driver", "executor", etc.) but learning this terminology may not be necessary if you are running Passim on a small dataset on a single machine.

Spark runs on Java 8, 11, or 17.  You can check whether you have one of these by typing the following command into a terminal window:

```bash
$ java -version
```

If the output of this command looks similar to the following example, then Java is installed on your machine.

```
openjdk version "1.8.0_262"
OpenJDK Runtime Environment (AdoptOpenJDK)(build 1.8.0_262-b10)
OpenJDK 64-Bit Server VM (AdoptOpenJDK)(build 25.262-b10, mixed mode)
```

If not, you will need to install Java.

On *macOS*, you can use the `brew` package manager.  (If necessary, install it by following installation instructions on the [Brew.sh](https://brew.sh/) website.)  Then run:

```bash
$ brew install java
```

On *Linux*, you can use other package managers such as `apt` and run something like this:

```bash
$ sudo apt install default-jre
```

Run the `java -version` command again to check your installation.

Now that you have a running `java` command, the easiest way to install Passim is use the `pip` program for installing Python modules. (On some machines, this program is named `pip3` to clarify that it works with Python version 3.)

You can have `pip` download Passim from GitHub directly and install it with:

```bash
$ pip install git+https://github.com/dasmiq/passim.git@seriatim#egg=passim
```

If `pip` complains that you don't have access to install python packages on the system, you can use the `--user` option to install in your home directory, like so:

```bash
$ pip install --user git+https://github.com/dasmiq/passim.git@seriatim#egg=passim
```

Alternately, you can clone the source code onto your own machine from the GitHub repository:

```bash
$ git clone https://github.com/dasmiq/passim.git --branch seriatim
```

If you are not familiar with Git and GitHub, we recommend reading the *Programming Historian* lesson ["An Introduction to Version Control Using GitHub Desktop"](https://doi.org/10.46430/phen0051).

Then, go into the `passim` directory and use pip to install it system-wide using this command:

```bash
$ pip install .
```

As above, you can add the `--user` option to install in your home directory.

In addition to installing the `passim` python package, this pip command installs command-line programs we will use below, including `passim`, `seriatim`, and `pyspark`.  The system-wide installation location may already be in your `PATH`.  You can check this with:

```bash
$ which passim
```

If you don't see anything, ask pip where it installed the `passim` package:

```bash
$ pip show --files passim
```

This should return something like:

```bash
Name: passim
Version: 2.0.0a2
Summary: Detecting and analyzing text reuse
Home-page: https://github.com/dasmiq/passim
Author: David A. Smith
Author-email: dasmiq@gmail.com
License: 
Location: /Users/dasmith/Library/Python/3.10/lib/python/site-packages
Requires: graphframes, intervaltree, pyspark
Required-by: 
Files:
  ../../../bin/passim
  ../../../bin/passim.cmd
  ../../../bin/seriatim
  ../../../bin/seriatim.cmd
  ../../../share/__pycache__/submit-seriatim.cpython-310.pyc
  ../../../share/submit-seriatim.py
  passim-2.0.0a2.dist-info/INSTALLER
  passim-2.0.0a2.dist-info/METADATA
  passim-2.0.0a2.dist-info/RECORD
  passim-2.0.0a2.dist-info/REQUESTED
  passim-2.0.0a2.dist-info/WHEEL
  passim-2.0.0a2.dist-info/direct_url.json
  passim-2.0.0a2.dist-info/top_level.txt
  passim/__init__.py
  passim/__pycache__/__init__.cpython-310.pyc
  passim/__pycache__/seriatim.cpython-310.pyc
  passim/seriatim.py
```

In the example above, pip has installed executable programs in `/Users/dasmith/Library/Python/3.10/bin/`.

To add the path permanently to the `PATH` environment variable, open the file `~/.bashrc` with your favorite text editor and add the following line anywhere in the file (then execute `source ~/.bashrc` to apply this change):

```bash
# replace "/Users/dasmith/Library/Python/3.10/bin" for the directory where you installed Passim
export PATH="/Users/dasmith/Library/Python/3.10/bin:$PATH"
```

## Verify the Installation

At this point you have installed Passim and all required packages on your machine. If you type `passim --help` in the command line, you should see output similar to the following:

```bash
https://repos.spark-packages.org/ added as a remote repository with the name: repo-1
:: loading settings :: url = jar:file:/usr/local/Cellar/apache-spark/3.3.0/libexec/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
Ivy Default Cache set to: /Users/dasmith/.ivy2/cache
The jars for the packages stored in: /Users/dasmith/.ivy2/jars
graphframes#graphframes added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-e8805593-a7b0-4c42-bd3b-02e872787256;1.0
	confs: [default]
	found graphframes#graphframes;0.8.0-spark3.0-s_2.12 in spark-packages
	found org.slf4j#slf4j-api;1.7.16 in central
:: resolution report :: resolve 225ms :: artifacts dl 4ms
	:: modules in use:
	graphframes#graphframes;0.8.0-spark3.0-s_2.12 from spark-packages in [default]
	org.slf4j#slf4j-api;1.7.16 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-e8805593-a7b0-4c42-bd3b-02e872787256
	confs: [default]
	0 artifacts copied, 2 already retrieved (0kB/6ms)
22/12/28 12:50:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
usage: submit-seriatim.py [-h] [-i ID] [-t TEXT] [-l N] [-u N] [-m N] [-n N]
                          [--floating-ngrams] [--complete-lines] [-g N]
                          [--max-offset N] [--beam N] [--pcopy p] [-a N]
                          [--src-overlap p] [--dst-overlap p]
                          [--fields FIELDS [FIELDS ...]] [-f FILTERPAIRS]
                          [--all-pairs] [--pairwise] [--docwise] [--linewise]
                          [--to-pairs] [--to-extents]
                          [--link-model LINK_MODEL]
                          [--link-features LINK_FEATURES]
                          [--log-level {ERROR,WARN,INFO,DEBUG}]
                          [--input-format INPUT_FORMAT]
                          [--output-format OUTPUT_FORMAT]
                          <path> <path>

Passim Alignment

positional arguments:
  <path>                input data
  <path>                output

optional arguments:
  -h, --help            show this help message and exit
  -i ID, --id ID        Field for unique document IDs (default: id)
  -t TEXT, --text TEXT  Field for document text (default: text)
  -l N, --minDF N       Lower limit on document frequency (default: 2)
  -u N, --maxDF N       Upper limit on document frequency (default: 100)
  -m N, --min-match N   Minimum number of n-gram matches between documents
                        (default: 5)
  -n N, --n N           n-gram order (default: 25)
  --floating-ngrams     Allow n-grams to float from word boundaries (default:
                        False)
  --complete-lines      Break target alignments at line breaks (default:
                        False)
  -g N, --gap N         Minimum size of gap that separates passages (default:
                        600)
  --max-offset N        Maximum offset in global alignment [deprecated]
                        (default: 20)
  --beam N              Beam search width (default: 20)
  --pcopy p             Probability of copying a character (default: 0.8)
  -a N, --min-align N   Minimum length of alignment (default: 50)
  --src-overlap p       Source overlap proportion (default: 0.9)
  --dst-overlap p       Destination overlap proportion (default: 0.5)
  --fields FIELDS [FIELDS ...]
                        List of fields to index (default: [])
  -f FILTERPAIRS, --filterpairs FILTERPAIRS
                        SQL constraint on posting pairs (default: uid < uid2)
  --all-pairs           Compute alignments for all pairs. (default: False)
  --pairwise            Output pairwise alignments (default: False)
  --docwise             Output docwise alignments (default: False)
  --linewise            Output linewise alignments (default: False)
  --to-pairs            Output pairs and stop (default: False)
  --to-extents          Output extents and stop (default: False)
  --link-model LINK_MODEL
                        Link model in R format (default: None)
  --link-features LINK_FEATURES
                        Link model features as SQL SELECT (default: None)
  --log-level {ERROR,WARN,INFO,DEBUG}
                        spark log level (default: WARN)
  --input-format INPUT_FORMAT
                        Input format (default: json)
  --output-format OUTPUT_FORMAT
                        Output format (default: json)
22/12/28 12:50:03 INFO ShutdownHookManager: Shutdown hook called
22/12/28 12:50:03 INFO ShutdownHookManager: Deleting directory /private/var/folders/d8/zzfwn8rs3mn9lwm_0cg7vlx40000gn/T/spark-f8eb403f-3ca1-4e32-9612-5c444e476a75
```

# Preparing Data for Passim

The goal of using Passim is to automate the search for repeated text passages in a corpus. For example, a newspaper corpus contains multiple copies of the same article, identical or with slight differences from one another, as well as repetitions of smaller portions of a newspaper page (e.g. advertisements, event listings, etc.).

As the documentation for Passim specifies "the input to Passim is a set of documents. Depending on the kind of data you have, you might choose documents to be whole books, pages of books, whole issues of newspapers, individual newspaper articles, etc. Minimally, a document consists of an identifier string and a single string of text content" (Refer to the minimal JSON input example in the next section for more information about the structure of input for Passim).

Figure 1 gives a schematic representation of input and output data for Passim. Given an input set of documents, divided into document series, Passim will attempt to identify reuse of text from documents in different series, and not within these series. In the case of a newspaper corpus, articles from the same newspaper will belong to the same document series, as we are not interested in detecting reuse within the same newspaper, but across different newspapers.

Ultimately, what constitutes a document, and how these documents should be divided into series, are the choices you'll need to make when preparing your data for Passim.  Naturally, the decision on what constitutes a *series* of documents is directly dependent on your goals or research questions. Finding quotations of the Bible in a corpus of books is a "one-to-many" case of text reuse detection, which requires documents to be grouped into two series (`bible` and `non_bible`). Instead, the comparison between multiple editions of the Bible (also known as collation) can be seen a "many-to-many" case, where each edition will correspond to and constitute a series of documents (e.g. pages).  If your research questions change at some point, thus requiring a re-definition of document series, you will need also to produce new input data for Passim to reflect this change.

{% include figure.html filename="textreuse-generic.png" caption="Figure 1. Schematic representation of text reuse clusters; each cluster consists of similar passages found in several series of documents." %}

## Basic JSON format

The input format for Passim consists of JSON documents in the [JSON lines format](http://jsonlines.org/) (i.e. each line of text contains a single JSON document).

The following file content for a file named `test.json` illustrates a minimal example of the input format for Passim: 

```json
{"id": "d1", "series": "abc", "text": "This is the text of a document."}
{"id": "d2", "series": "def", "text": "This is the text of another document."}
```

The fields `id`, `series` and `text` are the only fields required by Passim. Given this file as input, the software will try to detect text reuse between documents in the series `abc` and those in the series `def`, on the basis of the contents in `text`.

Throughout this tutorial we will be using the command-line tool [`jq`](https://stedolan.github.io/jq/) to inspect and do some basic process on both input and output JSON data. Note that, if you don't have `jq` installed, you'll need to execute `sudo apt-get install jq` under Ubuntu or `brew install jq` under macOS (for other operating systems [refer to the official JQ installation page](https://stedolan.github.io/jq/download/)).

For example, to select and print the field `series` of your input `test.json`, run the following command:

```bash
>>> jq '.series' test.json

# this will print
"abc"
"def"

```
Note: If you are using `jq` to look at your JSON data, you need to use the `--slurp` parameter whenever you want to treat the content of one or more JSON line files as a single array of JSON documents and apply some filters to it (e.g. to select and print only one document, use the following command `jq --slurp '.[-1]' test.json`). Otherwise `jq` will treat each document separately thus causing the following error:

```bash
>>> jq '.[0]' test.json

jq: error (at <stdin>:1): Cannot index string with string "series"
jq: error (at <stdin>:2): Cannot index string with string "series"

```
## A Note on Packaging Data

Depending one the total size of your data, it may be a good idea to store Passim input files as compressed archives. Passim supports several compression schemes like .gzip and .bzip2. Note that a compressed datastream will be slower to process than an uncompressed one, so using this option will only be beneficial if your data is large (i.e. gigabytes of text), if you have access to many computing cores, or have a limited amount of disk space.

This command (or, better, chain of commands) will output the first document in a bzip2-compressed JSON lines file (some fields have been truncated for the sake of readability):

```bash
$ bzcat impresso/data/GDL-1900.jsonl.bz2 | head -1 | jq '.'
```

And will output the following:
```json
{
  "series": "GDL",
  "date": "1900-07-07",
  "id": "GDL-1900-07-07-a-i0001",
  "cc": true,
  "lg": "fr",
    "pages": [
    {
      "id": "GDL-1900-07-07-a-p0001",
      "seq": 1,
      "regions": [
        {
          "start": 0,
          "length": 8,
          "coords": {
            "x": 206,
            "y": 1933,
            "w": 185,
            "h": 54
          }
        },
	{
          "start": 8,
          "length": 2,
          "coords": {
            "x": 388,
            "y": 1933,
            "w": 12,
            "h": 54
          }
        },
	...
       ]
     }
  ],
  "title": "Lausanne. 7. juillet 1900. BULLtTliMPÔLITIQUE",
  "text": "Lausanne. 7. juillet 1900.\nBULLtTliMPÔLITIQUE\nïiC rôle de l'Allemagne.\nDes deux discours de l'empereur ..."
}

```

# Running Passim

In this section we illustrate the usage of Passim with two separate case studies: 1) detecting Bible quotes in seventeent century texts and 2) detecting text reuse in a large corpus of historical newspapers. The first case study highlights some of the basics of using Passim,  while the second case study contains many details and best practices that would be helpful for a large-scale text reuse project.

In the following table, we build on the original Passim documentation and explain some of the more useful parameters that this library offers. The case studies do not require you to master these parameters, so feel free to skip directly to the [Downloading the Data](#downloading-the-data) section and come back to this section once you are comfortable enough to use Passim on your own data.

Parameter | Default value | Description | Explanation
--------- | ------------- | ----------- | -----------
`--n` | 25 | N-gram order for text-reuse detection | N-grams are chains of characters of length N. This setting allows you to decide what type of n-gram (unigram, bigram, trigram...) Passim should use when creating a list of possible text reuse candidates.<br /><br />Setting this parameter to a lower value can help in the case of very noisy texts (i.e. when many words in a text are affected by one or more OCR errors). In fact, the longer the n-gram, the more likely it is to contain OCR mistakes.
`--minDF` (`-l`) | 2 | Lower limit on document frequency of n-grams used | Since n-grams are used in Passim to retrieve document candidate pairs, an n-gram occurring only once is not useful as it will retrieve only one document (and not a pair). For this reason `--minDF` defaults to `2`.
`--maxDF` (`-u`)| 100 | Upper limit on document frequency of n-grams used. | This parameter will filter out n-grams that are too common, thus occurring many times in a given document. <br /><br />This value has an impact on the performances as it will reduce the number of document pairs retrieved by Passim that will need to be compared.
`--min-match` (`-m`)| 5 | Minimum number of matching n-grams between two documents | This parameter allows you to decide how many n-grams must be found between two documents.


## Downloading the data

Sample data needed to run the command examples in the two case studies can be downloaded from the [dedicated GitHub repository](https://github.com/dasmiq/PH-passim-tutorial). Before continuing with the case studies, download a local copy of the data by cloning the repository.

```bash
$ git clone https://github.com/dasmiq/PH-passim-tutorial.git
```

## Case study 1: Bible Quotes in Seventeenth Century Texts

In this first case study, we will look at text reuse using texts taken from [EEBO-TCP](https://textcreationpartnership.org/tcp-texts/eebo-tcp-early-english-books-online/) Phase I, the publicly available keyed-in version of Early English Books Online provided by the Text Creation Partnership. This case study is a special case of text reuse, as we are not focusing at inter-authors text reuse, but rather at the influence a single book — in this case, the Bible in its published-in-1611 King James version — had on several authors. Can we detect what documents contain extracts from the Bible?

As this is a small-scale example of what an actual research question making use of text reuse methods could look like, we will only use some of the 25,368 works available in EEBO-TCP, taken randomly. This smaller selection size should also allow anyone reading this tutorial to run this example on their personal laptop. Ideally, we recommend using a corpus such as [Early Modern Multiloquent Authors (EMMA)](https://www.uantwerpen.be/en/projects/mind-bending-grammars/emma-corpus/), compiled by the University of Antwerp's [Mind Bending Grammars](https://www.uantwerpen.be/en/projects/mind-bending-grammars/) project, should someone want to properly study the use of Bible quotes in seventeenth century texts. This corpus has the advantage of providing hand-curated metadata in an easily parseable format, allowing any researcher to focus on specific authors, periods, etc.

### Extracting the Data

At the root of the newly-created directory is a JSON file: `passim_in.json`. This file contains all our data, in the format described above: one document per line (`text`), structured with the bare minimum of required metadata (`id`, `series`). As this is a small file, we encourage you to open the file using a text editor such as Notepad++ on Windows or Sublime on Linux/macOS to familiarise yourself with how the data is formatted. Because our case study focuses on the detection of Bible passages in several documents and not on text reuse within all documents, we have formatted the data so that the `series` field contains `bible` for the Bible (last line of our JSON file), and `not_bible` for all other documents. Passim does not analyse documents that belong to the same series, so this effectively tells the software to only compare all documents with the Bible — not with each other.

The [accompanying Github repository](https://github.com/dasmiq/PH-passim-tutorial/) contains a [Python script](https://github.com/dasmiq/PH-passim-tutorial/blob/master/eebo/code/main.py) to transform EEBO-TCP into the JSON format required by Passim and used in this lesson. We encourage the readers to reuse it and adapt it to their needs.

### Running Passim

Create a directory where you want to store the output of Passim (we use `passim_output_bible` but any name will work). If you decide to use the default `passim_output_bible` directory, ensure you remove all of its content (i.e. pre-computed Passim output) either manually or by running `rm -r ./eebo/passim_output_bible/*`.

You are now ready to go forward with your first text reuse project. 

1. Move to the sub-directory `eebo` by executing the command `cd eebo/`, starting from the directory where, earlier on, you cloned the repository [`PH-Passim-tutorial`](https://github.com/dasmiq/PH-passim-tutorial/).

2. Run the following command and go have a cup of your favorite hot beverage:
```bash
$ passim passim_in.json passim_output_bible/
```
This test case takes approximatively eight minutes on a recent laptop with eight threads. You can also follow the progress of the detection at http://localhost:4040 — an interactive dashboard created by Spark (Note: the dashboard will shut down as soon as Passim has finished running).

As we will see in more detail in the [second use case](#case-study-2:-text-reuse-in-a-large-corpus-of-historical-newspapers), Passim, through Spark, allows for many options. By default Java might not allocate much memory to its processes, and running Passim even on very little datasets can cause Passim to crash because of an `OutOfMemory` error — even if you have a machine with a lot of RAM. If you see errors like this, you can call Passim with some additional parameters that will tell Spark to use more RAM for its processes.

```bash
$ SPARK_SUBMIT_ARGS='--master local[12] --driver-memory 8G --executor-memory 4G' passim passim_in.json passim_output_bible/
```

## Case study 2: Text Reuse in a large corpus of historical newspapers

The second case study is drawn from [impresso](https://impresso-project.ch/), a recent research project aimed at enabling critical text mining of newspaper archives with the implementation of a technological framework to extract, process, link, and explore data from print media archives.

In this project, we use Passim to detect text reuse at scale. The extracted text reuse clusters are then integrated into the [impresso tool](https://impresso-project.ch/app) in two ways. First, in the main article reading view users can readily see which portions of an article were reused by other articles in the corpus. Second, users can browse through all clusters in a dedicated page (currently more than 6 million), perform full-text searches on their contents, and filter the results according to a number of criteria (cluster size, time span covered, lexical overlap, etc.).

More generally, detecting text reuse in a large-scale newspaper corpus can be useful in many of the following ways:
* Identify (and possibly filter out) duplicated documents before performing further processing steps (e.g. topic modelling)
* Study the virality and spread of news
* Study information flows, both within and across national borders
* to allow users discover which contents, within in their own collections, generated text reuse (e.g. famous political speeches, portions of national constitutions, etc.)

For this case study we consider a tiny fraction of the *impresso* corpus, consisting of one year's worth of newspaper data (i.e. 1900) for a sample of four newspapers. The corpus contains 76 newspapers from Switzerland and Luxembourg, covering a time span of 200 years. The sample data necessary to run step by step this case study are contained in the folder [`impresso/`](https://github.com/dasmiq/PH-passim-tutorial/tree/master/impresso).

### Data preparation

The format used in impresso to store newspapers data is slightly different from Passim's input format so we need a script to take care of transforming the former into the latter. While discussing how this script works goes well beyond the scope of this lesson, you can find the conversion script on the [impresso GitHub repository](https://github.com/impresso/impresso-pycommons/blob/master/impresso_commons/text/rebuilder.py) should you be interested. The output of this script is one JSON line file per newspaper per year, compressed into a `.bz2` archive for the sake of efficient storage. Examples of this format can be found in the directory `impresso/data` and shown in the following example:

```bash
$ ls -la impresso/data/
EXP-1900.jsonl.bz2
GDL-1900.jsonl.bz2
IMP-1900.jsonl.bz2
JDG-1900.jsonl.bz2
```

Each newspaper archive is named after the newspaper identifier: for example, `GDL` stands for *Gazette de Lausanne*. In total, these four `.bz2` files contain 92,000 articles through Passim, corresponding to all articles published in 1900 in the four sampled newspapers.

Sometimes it's not easy to inspect data packaged in this way. But some Bash commands like `bzcat` and `jq` can help us. For example, with the following chain of commands we can find out how many documents (newspaper articles) are contained in each of the input files by counting their IDs:

```bash
$ bzcat impresso/data/GDL-1900.jsonl.bz2 | jq --slurp '[.[] |del(.pages)| .id]|length'
28380
```

And similarly, in all input files:

```bash
$ bzcat impresso/data/*-1900.jsonl.bz2 | jq --slurp '[.[] |del(.pages)| .id]|length'
92514
```

What these commands do is to read the content of the `.bz2` file by means of `bzcat` and then *pipe* this content into `jq` which
- iterates through all docouments in the JSON line file
- for each document it removes the `pages` field as it's not needed and selects only the `id` field
- finally, with `length` `jq` computes the size of the list of IDs created by the previous expression

### Running Passim

To run the impresso data through Passim, execute the following command in a `Terminal` window:

```bash
$ passim impresso/data impresso/passim-output/
```

On our machine, Java only allocates 1GB of memory by default, and this takes 25 minutes.  As you work with more data, you might want to take advantage of more processor cores and more memory.  You can pass this informaiton to the Spark runtime using an environment variable like so:

```bash
$ SPARK_SUBMIT_ARGS='--master local[12] --driver-memory 10G --executor-memory 10G --conf spark.local.dir=/scratch/matteo/spark-tmp/' passim impresso/data" "impresso/passim-output/"
```

This command is made up of the following parameters:
- **`SPARK_SUBMIT_ARGS`** passes some configuration parameters to Spark, the library that takes care of parallel execution of processes.
    - `--master local[12]`: `local` means we are running Spark in single machine-mode; `[12]` specifies the number of workers (or threads, in this specific case) over which processes should be distributed (`local [*]` will make use of the maximum number of threads);  
    - `--executor-memory 10G`: The equivalent of the maximum heap size when running a regular JAVA application. It's the amount of memory that Spark allocates to each executor.
    - `--conf spark.local.dir=/scratch/matteo/spark-tmp/`: A directory where Spark stores temporary data. When working with large datasets, it is important to specify a location with sufficient free disk space.

If you want to limit the processing to a couple of input files — for example to limit memory usage — you can specify the input using the following command:

```
$ passim "impresso/data/{EXP,GDL}-1900.jsonl.bz2" impresso/papers
```

The quotes are important, since they cause the wildcard filenames to be expanded by Passim and not by the shell.

You can monitor Passim's progress while running by pointing your browser to the address `localhost:4040` where the Spark dashboard can be accessed (Figure 2).

{% include figure.html filename="spark-dashboard.png" caption="Figure 2. Screenshot of the Spark dashboard while running Passim." %}

Running Passim with eight workers (and 4 Gb of executor memory) takes about five minutes to process the 92,514 articles published in 1900 in the newspapers GDL, JDG, EXP, IMP (but your mileage may vary).

It is important that the output folder where Passim will write its output is empty. Especially when running the first experiments and getting familiar with the software it can very easily happen to specify a non-empty output folder. Specifying a non-empty output folder usually leads to an error as Passim processes the folder content and does not simply overwrite it.

### Inspecting Passim's Output

Once Passim has finished running, the output folder `impresso/passim-output/` will contain a sub-folder `out.json/` with the extracted text reuse clusters. If you specified `--output=parquet` instead of `--output=json`, this sub-folder will be named `out.parquet`.

In the JSON output each document corresponds to a text reuse passage. Since passages are aggregated into clusters, each passage contains a field `cluster` with the ID of the cluster to which it belongs.

To obtain the total number of cluster, we can count the number of unique cluster IDs with the following one-line command:


```bash
$ cat impresso/passim-output/out.json/*.json | jq -c .cluster | sort -u | wc -l'
18328
```

Similarly, we can print the 5000th cluster ID:

```bash
$ cat impresso/passim-output/out.json/*.json | jq -c .cluster | uniq | head -5000 | tail -1
6056
```

And with a simple `jq` query we can print all passages belonging to this text reuse cluster:

```bash
$ cat impresso/passim-output/out.json/*.json | jq 'select(.cluster == 6056)' 
```

```json
{
  "uid": -3975303822424114000,
  "cluster": 6056,
  "begin": 5,
  "end": 417,
  "boiler": false,
  "size": 3,
  "pboiler": 0,
  "cc": true,
  "date": "1900-10-22",
  "id": "GDL-1900-10-22-a-i0020",
  "lg": "fr",
  "series": "GDL",
  "text": "ÉDÉRATION SUISSE\nAnarchistes. — Suivant renseignements pris\nà bonne source par l'Agence télégraphique suis-\nse, la nouvelle pnbliée par la presse étrangère,\nsuivant laquelle certaines puissances auraient\ndemandé au Conseil fédéral s'il consentirait à\nprendre des, mesures spéciales contre les anar-\nchistes, est fausse. Le Conseil fédéral n'a plus\nreçu de communication à ce snjet depuis la con-\nférence de Rome.",
  "title": "CONFÉDÉRATION SUISSE"
}
{
  "uid": -2433681819068420000,
  "cluster": 6056,
  "begin": 10,
  "end": 421,
  "boiler": false,
  "size": 3,
  "pboiler": 0,
  "cc": true,
  "date": "1900-10-21",
  "id": "JDG-1900-10-21-a-i0028",
  "lg": "fr",
  "series": "JDG",
  "text": " DÉPÊCHES\nSuisse\nBerne, 20. — Suivant des renseignements\npris à bonne source, la nouvelle publiée par\nla presse suisse et étrangère, suivant laquelle\ncertaines puissances auraient demandé au\nConseil fédéral s'il se prêterait à prendre des\nmesures spéciales contre les anarchistes, est\ncomplètement dénuée de fondement. Le Con-\nseil fédéral n'a plus reçu de communication\nà ce sujet depuis la conférence de Rome.",
  "title": "DERNIÈRES DÉPÊCHES"
}
{
  "uid": -5145029376058709000,
  "cluster": 6056,
  "begin": 21,
  "end": 431,
  "boiler": false,
  "size": 3,
  "pboiler": 0,
  "cc": true,
  "date": "1900-10-22",
  "id": "EXP-1900-10-22-a-i0042",
  "lg": "fr",
  "series": "EXP",
  "text": "_ELLES\nBerne, 20 octobre.\nSuivant des renseignements pris à\nbonne source, la nouvelle publiée par la\npresse suisse et étrangère, suivant la-\nquelle certaines puissances auraient de-\nmandé au Conseil fédéral s'il se prêterait\nà prendre des mesures spéciales contre\nles anarchistes, est complètement dé-\nnuée de fondement Le Conseil fédéral\nn'a plus reçu de communications à ce\nsujet depuis la conférence de Rome",
  "title": "slSRhlEEES M0U¥ELLES"
}
```

As you can see from the output above, this cluster contains the same piece of news — a denial by the Swiss Federal Council that it was asked to target anarchists — reported by three different newspapers on the very same day with slightly different words.

# Using Passim's Output

Since the usage of text reuse data ultimately depends on the research questions at hand — and there many possible applications of text reuse, as we have seen above — covering how to use Passim's output falls beyond the scope of this lesson.

Code that 'does something' with the data output by Passim can be written in many different programming languages. Extracted clusters can be used to deduplicate documents in a corpus, or even collate together multiple witnesses of the same text, but this will entirely depend on the research context and specific use case.

To given an example of where to go next, for those who want to manipulate and further analyse text reuse data in Python, we provide a Jupyter notebook ([`explore-Passim-output.ipynb`](https://github.com/dasmiq/PH-passim-tutorial/blob/master/explore-Passim-output.ipynb)) that shows how to import Passim's JSON output into a `pandas.DataFrame` and how to analyse the distribution of text reuse clusters in both uses cases presented above. For readers that are not familair with the Python library `pandas`, the *Programming Historian* lesson written by Charlie Harper on [*Visualizing Data with Bokeh and Pandas*](https://programminghistorian.org/en/lessons/visualizing-with-bokeh) is a nice (and required) introductory reading.

The code contained and explained in the notebook will produce the two plots of Figures 3 and 4, showing how the sizes of text reuse clusters are distributed in the impresso and Bible data respectively.


{% include figure.html filename="plot-impresso.png" caption="Figure 3. Distribution of text reuse cluster sizes in the impresso sample data." %}

{% include figure.html filename="plot-bible.png" caption="Figure 4. Distribution of text reuse cluster sizes in the Bible sample data." %}

As you can see from the plots, in both cases the majority of text reuse clusters contains at most two passages. In the impresso sample data, however, there is much more variance in the size of clusters, with 10% of them having a size comprised between 6 and 296 passages, as opposed to the Bible data where the maximum cluster size is 3.

# Further readings

**Passim**
- Smith et al. (2015) introduce in detail the text reuse detection algorithm implemented in Passim
- Cordell (2015) applied Passim to study text reuse within a large corpus of American newspapers

**textreuse**

- Vogler et al. (2020) apply the `textreuse` R package (Mullen 2016) to study the phenomenon of *media concentration* in contemporary journalism

**TRACER**
- Büchler et al. (2014) explain the algorithms for text reuse detection that are implemented in TRACER;
- Franzini et al. (2018) use and evaluate TRACER for the extraction of quotations from a Latin text (the *Summa contra Gentiles* of Thomas Aquinas)

**BLAST**
- Vierthaler et al. (2019) use the BLAST alignment algorithm to detect reuse in Chinese texts
- Vesanto et al. (2017) and Salmi et al. (2019) apply BLAST to a comprehensive corpus of newspapers published in Finland

# Acknowledgements

A sincere thanks goes to Marco Büchler and Ryan Muther for reviewing this lesson, as well as to our colleagues Marten Düring and David Smith for their constructive feedback on an early version of this tutorial. Additional thanks go to Anna-Maria Sichani for serving as editor.

The authors warmly thank the newspaper [Le Temps](https://letemps.ch/) — owner of *La Gazette de Lausanne* (GDL) and the *Journal de Genève* (JDG) — and the group [ArcInfo](https://www.arcinfo.ch/) — owner of *L’Impartial* (IMP) and *L’Express* (EXP) —  for accepting to share their data for academic purposes.

MR gratefully acknowledges the financial support of the Swiss National Science Foundation (SNSF) for the project [*impresso – Media Monitoring of the Past*](https://impresso-project.ch/) under grant number CR-SII5_173719. SH's work was supported by the European Union’s Horizon 2020 research and innovation programme under grant 770299 ([NewsEye](https://www.newseye.eu/)). SH was affiliated with the University of Helsinki and the University of Geneva for most of this work, and is currently funded by the project *Towards Computational Lexical Semantic Change Detection* supported by the Swedish Research Council (20192022; dnr 2018-01184).

# Bibliography

1. Greta Franzini, Maria Moritz, Marco Büchler, Marco Passarotti. Using and evaluating TRACER for an Index fontium computatus of the Summa contra Gentiles of Thomas Aquinas. In *Proceedings of the Fifth Italian Conference on Computational Linguistics (CLiC-it 2018)*. (2018). [Link](http://ceur-ws.org/Vol-2253/paper22.pdf)
2. David A. Smith, Ryan Cordell, Abby Mullen. Computational Methods for Uncovering Reprinted Texts in Antebellum Newspapers. *American Literary History* **27**, E1–E15 Oxford University Press, 2015. [Link](http://dx.doi.org/10.1093/alh/ajv029)
3. Ryan Cordell. Reprinting Circulation, and the Network Author in Antebellum Newspapers. *American Literary History* **27**, 417–445 Oxford University Press (OUP), 2015. [Link](http://dx.doi.org/10.1093/alh/ajv028)
4. Daniel Vogler, Linards Udris, Mark Eisenegger. Measuring Media Content Concentration at a Large Scale Using Automated Text Comparisons. *Journalism Studies* **0**, 1–20 Taylor & Francis, 2020. [Link](http://dx.doi.org/10.1080/1461670x.2020.1761865)
5. Lincoln Mullen. textreuse: Detect Text Reuse and Document Similarity. (2016). [Link](https://github.com/ropensci/textreuse)
6. Marco Büchler, Philip R. Burns, Martin Müller, Emily Franzini, Greta Franzini. Towards a Historical Text Re-use Detection. 221–238 In *Text Mining: From Ontology Learning to Automated Text Processing Applications*. Springer International Publishing, 2014. [Link](http://dx.doi.org/10.1007/978-3-319-12655-5_11)
8. Paul Vierthaler, Meet Gelein. A BLAST-based, Language-agnostic Text Reuse Algorithm with a MARKUS Implementation and Sequence Alignment Optimized for Large Chinese Corpora. *Journal of Cultural Analytics* (2019). [Link](http://dx.doi.org/10.22148/16.034)
9. Aleksi Vesanto, Asko Nivala, Heli Rantala, Tapio Salakoski, Hannu Salmi, Filip Ginter. Applying BLAST to Text Reuse Detection in Finnish Newspapers and Journals, 1771-1910. 54–58 In *Proceedings of the NoDaLiDa 2017 Workshop on Processing Historical Language*. Linköping University Electronic Press, 2017. [Link](https://www.aclweb.org/anthology/W17-0510)
10. Hannu Salmi, Heli Rantala, Aleksi Vesanto, Filip Ginter. The long-term reuse of text in the Finnish press, 1771–1920. **2364**, 394–544 In *CEUR Workshop Proceedings*. (2019).
11. Axel J Soto, Abidalrahman Mohammad, Andrew Albert, Aminul Islam, Evangelos Milios, Michael Doyle, Rosane Minghim, Maria Cristina de Oliveira. Similarity-Based Support for Text Reuse in Technical Writing. 97–106 In *Proceedings of the 2015 ACM Symposium on Document Engineering*. ACM, 2015. [Link](http://dx.doi.org/10.1145/2682571.2797068)
12. Alexandra Schofield, Laure Thompson, David Mimno. Quantifying the Effects of Text Duplication on Semantic Models. 2737–2747 In *Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing*. Association for Computational Linguistics, 2017. [Link](http://dx.doi.org/10.18653/v1/D17-1290)
13. Matteo Romanello, Aurélien Berra, Alexandra Trachsel. Rethinking Text Reuse as Digital Classicists. *Digital Humanities conference*, 2014. [Link](http://dharchive.org/paper/DH2014/Panel-106.xml)
